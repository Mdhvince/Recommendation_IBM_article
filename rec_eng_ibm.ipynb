{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import recommender as r\n",
    "\n",
    "# Nlp\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import spacy\n",
    "#nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Viz\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data analysis + Cleaning steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('user-item-interactions.csv')\n",
    "df_content = pd.read_csv('articles_community.csv')\n",
    "del df['Unnamed: 0']\n",
    "del df_content['Unnamed: 0']\n",
    "\n",
    "# make sure all articles from df are in df_content\n",
    "df = pd.merge(df, df_content[['article_id']], on='article_id', how='inner')\n",
    "\n",
    "df_content = df_content.drop(labels='doc_status', axis=1)\n",
    "df_content = df_content.drop_duplicates()\n",
    "\n",
    "#df = df.replace('no_email', np.nan)\n",
    "\n",
    "def email_mapper():\n",
    "    coded_dict = dict()\n",
    "    cter = 1\n",
    "    email_encoded = []\n",
    "    \n",
    "    for val in df['email']:\n",
    "        if val not in coded_dict:\n",
    "            coded_dict[val] = cter\n",
    "            cter+=1\n",
    "        \n",
    "        email_encoded.append(coded_dict[val])\n",
    "    return email_encoded\n",
    "\n",
    "email_encoded = email_mapper()\n",
    "del df['email']\n",
    "df['user_id'] = email_encoded\n",
    "\n",
    "# Rename my article name field to be the same in both dfs\n",
    "df_content['title'] = df_content['doc_full_name']\n",
    "df_content = df_content.drop(labels='doc_full_name', axis=1)\n",
    "\n",
    "df.article_id = df.article_id.astype('int64')\n",
    "df_content.article_id = df_content.article_id.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>593</td>\n",
       "      <td>upload files to ibm data science experience us...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>593</td>\n",
       "      <td>upload files to ibm data science experience us...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>593</td>\n",
       "      <td>upload files to ibm data science experience us...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>593</td>\n",
       "      <td>upload files to ibm data science experience us...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>593</td>\n",
       "      <td>upload files to ibm data science experience us...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                              title  user_id\n",
       "0         593  upload files to ibm data science experience us...        1\n",
       "1         593  upload files to ibm data science experience us...        2\n",
       "2         593  upload files to ibm data science experience us...        3\n",
       "3         593  upload files to ibm data science experience us...        4\n",
       "4         593  upload files to ibm data science experience us...        3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_body</th>\n",
       "      <th>doc_description</th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Skip navigation Sign in SearchLoading...\\r\\n\\r...</td>\n",
       "      <td>Detect bad readings in real time using Python ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Detect Malfunctioning IoT Sensors with Streami...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No Free Hunch Navigation * kaggle.com\\r\\n\\r\\n ...</td>\n",
       "      <td>See the forest, see the trees. Here lies the c...</td>\n",
       "      <td>1</td>\n",
       "      <td>Communicating data science: A guide to present...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>☰ * Login\\r\\n * Sign Up\\r\\n\\r\\n * Learning Pat...</td>\n",
       "      <td>Here’s this week’s news in Data Science and Bi...</td>\n",
       "      <td>2</td>\n",
       "      <td>This Week in Data Science (April 18, 2017)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DATALAYER: HIGH THROUGHPUT, LOW LATENCY AT SCA...</td>\n",
       "      <td>Learn how distributed DBs solve the problem of...</td>\n",
       "      <td>3</td>\n",
       "      <td>DataLayer Conference: Boost the performance of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Skip navigation Sign in SearchLoading...\\r\\n\\r...</td>\n",
       "      <td>This video demonstrates the power of IBM DataS...</td>\n",
       "      <td>4</td>\n",
       "      <td>Analyze NY Restaurant data using Spark in DSX</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            doc_body  \\\n",
       "0  Skip navigation Sign in SearchLoading...\\r\\n\\r...   \n",
       "1  No Free Hunch Navigation * kaggle.com\\r\\n\\r\\n ...   \n",
       "2  ☰ * Login\\r\\n * Sign Up\\r\\n\\r\\n * Learning Pat...   \n",
       "3  DATALAYER: HIGH THROUGHPUT, LOW LATENCY AT SCA...   \n",
       "4  Skip navigation Sign in SearchLoading...\\r\\n\\r...   \n",
       "\n",
       "                                     doc_description  article_id  \\\n",
       "0  Detect bad readings in real time using Python ...           0   \n",
       "1  See the forest, see the trees. Here lies the c...           1   \n",
       "2  Here’s this week’s news in Data Science and Bi...           2   \n",
       "3  Learn how distributed DBs solve the problem of...           3   \n",
       "4  This video demonstrates the power of IBM DataS...           4   \n",
       "\n",
       "                                               title  \n",
       "0  Detect Malfunctioning IoT Sensors with Streami...  \n",
       "1  Communicating data science: A guide to present...  \n",
       "2         This Week in Data Science (April 18, 2017)  \n",
       "3  DataLayer Conference: Boost the performance of...  \n",
       "4      Analyze NY Restaurant data using Spark in DSX  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_content.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>user_id</th>\n",
       "      <th>nb_interactions_user_article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>593</td>\n",
       "      <td>upload files to ibm data science experience us...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>593</td>\n",
       "      <td>upload files to ibm data science experience us...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>593</td>\n",
       "      <td>upload files to ibm data science experience us...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>593</td>\n",
       "      <td>upload files to ibm data science experience us...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>593</td>\n",
       "      <td>upload files to ibm data science experience us...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                              title  user_id  \\\n",
       "0         593  upload files to ibm data science experience us...        1   \n",
       "1         593  upload files to ibm data science experience us...        2   \n",
       "2         593  upload files to ibm data science experience us...        3   \n",
       "3         593  upload files to ibm data science experience us...        4   \n",
       "4         593  upload files to ibm data science experience us...        5   \n",
       "\n",
       "   nb_interactions_user_article  \n",
       "0                             1  \n",
       "1                             1  \n",
       "2                             2  \n",
       "3                             1  \n",
       "4                             1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of times the user seen the article\n",
    "pre_data = dict(df.groupby(['user_id', 'article_id'])['article_id'].count())\n",
    "\n",
    "list_user = []\n",
    "list_article_id = []\n",
    "list_nb_interactions = []\n",
    "\n",
    "for key, val in pre_data.items():\n",
    "    list_user.append(key[0])\n",
    "    list_article_id.append(key[1])\n",
    "    list_nb_interactions.append(val)\n",
    "    \n",
    "zipped_list = list(zip(list_user, list_article_id, list_nb_interactions))\n",
    "interaction_user = pd.DataFrame(zipped_list, columns=['user_id','article_id','nb_interactions_user_article'])\n",
    "\n",
    "df = pd.merge(df, interaction_user, on=['user_id','article_id'])\n",
    "\n",
    "# The nb interactions was shown implicitly with the number of rows\n",
    "# Now I have information about the number of interactions by creating this column\n",
    "# I can drop duplicated rows\n",
    "df = df.drop_duplicates(keep='first')\n",
    "df = df.reset_index().drop(labels='index', axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I don't have any date of the interraction between user and article, so just for the module, I'll simulate a date field set to 0. Popularity of an item is computed by the weighted rating calcul shown in recommender_function.py, but in case of tie, the more recent date will be first, so by putting a same value for all the records, I'm not influence the importance of an article. Here I chose 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_pipeline(df_reviews, df_items):\n",
    "    \n",
    "    df = df_reviews.copy()\n",
    "    df_content = df_items.copy()\n",
    "    \n",
    "    if 'Unnamed: 0' in df.columns:\n",
    "        del df['Unnamed: 0']\n",
    "        \n",
    "    if 'Unnamed: 0' in df_content.columns:\n",
    "        del df_content['Unnamed: 0']\n",
    "\n",
    "    # make sure all articles from df are in df_content\n",
    "    df = pd.merge(df, df_content[['article_id']], on='article_id', how='inner')\n",
    "\n",
    "    df_content = df_content.drop(labels='doc_status', axis=1)\n",
    "    df_content = df_content.drop_duplicates()\n",
    "\n",
    "    #df = df.replace('no_email', np.nan)\n",
    "\n",
    "    def email_mapper():\n",
    "        coded_dict = dict()\n",
    "        cter = 1\n",
    "        email_encoded = []\n",
    "\n",
    "        for val in df['email']:\n",
    "            if val not in coded_dict:\n",
    "                coded_dict[val] = cter\n",
    "                cter+=1\n",
    "\n",
    "            email_encoded.append(coded_dict[val])\n",
    "        return email_encoded\n",
    "\n",
    "    email_encoded = email_mapper()\n",
    "    del df['email']\n",
    "    df['user_id'] = email_encoded\n",
    "\n",
    "    # Rename my article name field to be the same in both dfs\n",
    "    df_content['title'] = df_content['doc_full_name']\n",
    "    df_content = df_content.drop(labels='doc_full_name', axis=1)\n",
    "\n",
    "    df.article_id = df.article_id.astype('int64')\n",
    "    df_content.article_id = df_content.article_id.astype('int64')\n",
    "\n",
    "    # number of times the user seen the article\n",
    "    pre_data = dict(df.groupby(['user_id', 'article_id'])['article_id'].count())\n",
    "\n",
    "    list_user = []\n",
    "    list_article_id = []\n",
    "    list_nb_interactions = []\n",
    "\n",
    "    for key, val in pre_data.items():\n",
    "        list_user.append(key[0])\n",
    "        list_article_id.append(key[1])\n",
    "        list_nb_interactions.append(val)\n",
    "\n",
    "    zipped_list = list(zip(list_user, list_article_id, list_nb_interactions))\n",
    "    interaction_user = pd.DataFrame(zipped_list, columns=['user_id','article_id','nb_interactions_user_article'])\n",
    "\n",
    "    df = pd.merge(df, interaction_user, on=['user_id','article_id'])\n",
    "\n",
    "    # The nb interactions was shown implicitly with the number of rows\n",
    "    # Now I have information about the number of interactions by creating this column\n",
    "    # I can drop duplicated rows\n",
    "    df = df.drop_duplicates(keep='first')\n",
    "    df = df.reset_index().drop(labels='index', axis=1)\n",
    "\n",
    "    df['date'] = 0\n",
    "    \n",
    "    return df, df_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('user-item-interactions.csv')\n",
    "df_content = pd.read_csv('articles_community.csv')\n",
    "\n",
    "df, df_content = clean_pipeline(df_reviews=df,\n",
    "                                df_items=df_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare TFIDF Matrice for Content Based Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    This function do the following steps:\n",
    "    - Normalize, remove ponctuations\n",
    "    - Tokenize the input (str)\n",
    "    - Remove english stopwords\n",
    "    - Lemmatize but keep the gramatical sense of the word\n",
    "    - Clean White spaces\n",
    "    \"\"\"\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
    "    words = word_tokenize(text)\n",
    "    words = [w for w in words if w not in stopwords.words(\"english\")]\n",
    "    lemmed = [WordNetLemmatizer().lemmatize(w, pos='v') for w in words]\n",
    "    words_clean = [word.strip() for word in words]\n",
    "    return words_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=tokenize, ngram_range=(1, 2))\n",
    "df_content['doc_description'] = df_content['doc_description'].fillna('')\n",
    "tfidf_matrix = tfidf.fit_transform(df_content['doc_description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare User-Item Matrix to feed my recommender object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_user_item():\n",
    "    user_item = df[['user_id',\n",
    "                    'article_id',\n",
    "                    'nb_interactions_user_article']]\n",
    "    \n",
    "    user_item_df=(\n",
    "        user_item.groupby(['user_id',\n",
    "                           'article_id'])['nb_interactions_user_article'].max().unstack()\n",
    "    )\n",
    "    \n",
    "    return user_item_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_df = create_user_item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec = r.Recommender(df_items=df_content,                            # df that contains all the items with description and more\n",
    "                    df_reviews=df,                                  # df that contains interactions between users and items\n",
    "                    user_item_df=user_item_df,                      # A user item df\n",
    "                    item_name_colname='title',                      # The title column of the df (this can be use with the 1st df or the 2nd, that why I wanted the same name for both)\n",
    "                    user_id_colname='user_id',                      # The name of the user id column\n",
    "                    item_id_colname='article_id',                   # The name of the item id column\n",
    "                    rating_col_name='nb_interactions_user_article', # The rating column\n",
    "                    date_col_name='date')                           # The date column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data with Funk Sigular Value Decomposition...\n",
      "Iterations \t\t Mean Squared Error \n",
      "\t1 \t\t 0.6623863732481259 \n",
      "\t2 \t\t 0.6544580615101401 \n",
      "\t3 \t\t 0.6470075236738265 \n",
      "\t4 \t\t 0.6399923516660931 \n",
      "\t5 \t\t 0.6333746904568995 \n",
      "\t6 \t\t 0.627120691241326 \n",
      "\t7 \t\t 0.6212000355791115 \n",
      "\t8 \t\t 0.6155855206583186 \n",
      "\t9 \t\t 0.6102526973050068 \n",
      "\t10 \t\t 0.6051795535851098 \n",
      "\t11 \t\t 0.6003462378771757 \n",
      "\t12 \t\t 0.5957348161680779 \n",
      "\t13 \t\t 0.5913290590641385 \n",
      "\t14 \t\t 0.5871142546395772 \n",
      "\t15 \t\t 0.5830770437798646 \n",
      "\t16 \t\t 0.5792052751353941 \n",
      "\t17 \t\t 0.5754878771915503 \n",
      "\t18 \t\t 0.5719147452967154 \n",
      "\t19 \t\t 0.5684766417771546 \n",
      "\t20 \t\t 0.5651651075148402 \n",
      "\t21 \t\t 0.5619723835771527 \n",
      "\t22 \t\t 0.5588913416708042 \n",
      "\t23 \t\t 0.5559154223506516 \n",
      "\t24 \t\t 0.5530385800510309 \n",
      "\t25 \t\t 0.5502552341257603 \n",
      "\t26 \t\t 0.5475602251856406 \n",
      "\t27 \t\t 0.5449487761112689 \n",
      "\t28 \t\t 0.5424164571966155 \n",
      "\t29 \t\t 0.5399591549459256 \n",
      "\t30 \t\t 0.537573044105049 \n",
      "\t31 \t\t 0.535254562559597 \n",
      "\t32 \t\t 0.5330003887765046 \n",
      "\t33 \t\t 0.5308074215046945 \n",
      "\t34 \t\t 0.5286727614843448 \n",
      "\t35 \t\t 0.5265936949440595 \n",
      "\t36 \t\t 0.5245676786911798 \n",
      "\t37 \t\t 0.5225923266232283 \n",
      "\t38 \t\t 0.5206653975085983 \n",
      "\t39 \t\t 0.5187847839018304 \n",
      "\t40 \t\t 0.5169485020746412 \n",
      "\t41 \t\t 0.5151546828569811 \n",
      "\t42 \t\t 0.5134015632946044 \n",
      "\t43 \t\t 0.5116874790399591 \n",
      "\t44 \t\t 0.5100108574025801 \n",
      "\t45 \t\t 0.5083702109931171 \n",
      "\t46 \t\t 0.5067641319026364 \n",
      "\t47 \t\t 0.5051912863648207 \n",
      "\t48 \t\t 0.5036504098546323 \n",
      "\t49 \t\t 0.5021403025817888 \n",
      "\t50 \t\t 0.5006598253419366 \n",
      "\t51 \t\t 0.499207895692083 \n",
      "\t52 \t\t 0.4977834844206988 \n",
      "\t53 \t\t 0.4963856122855391 \n",
      "\t54 \t\t 0.4950133469953141 \n",
      "\t55 \t\t 0.4936658004135479 \n",
      "\t56 \t\t 0.49234212596527377 \n",
      "\t57 \t\t 0.4910415162290512 \n",
      "\t58 \t\t 0.4897632006984722 \n",
      "\t59 \t\t 0.4885064436991232 \n",
      "\t60 \t\t 0.4872705424478526 \n",
      "\t61 \t\t 0.48605482524294635 \n",
      "\t62 \t\t 0.4848586497745872 \n",
      "\t63 \t\t 0.4836814015459573 \n",
      "\t64 \t\t 0.48252249239648753 \n",
      "\t65 \t\t 0.4813813591191539 \n",
      "\t66 \t\t 0.480257462164851 \n",
      "\t67 \t\t 0.47915028442709495 \n",
      "\t68 \t\t 0.4780593301013139 \n",
      "\t69 \t\t 0.4769841236130595 \n",
      "\t70 \t\t 0.47592420861034135 \n",
      "\t71 \t\t 0.47487914701544415 \n",
      "\t72 \t\t 0.47384851813200074 \n",
      "\t73 \t\t 0.4728319178036328 \n",
      "\t74 \t\t 0.4718289576205337 \n",
      "\t75 \t\t 0.4708392641707334 \n",
      "\t76 \t\t 0.46986247833319456 \n",
      "\t77 \t\t 0.46889825460990037 \n",
      "\t78 \t\t 0.4679462604943809 \n",
      "\t79 \t\t 0.4670061758744485 \n",
      "\t80 \t\t 0.46607769246687336 \n",
      "\t81 \t\t 0.46516051328206864 \n",
      "\t82 \t\t 0.4642543521168715 \n",
      "\t83 \t\t 0.46335893307379544 \n",
      "\t84 \t\t 0.4624739901050175 \n",
      "\t85 \t\t 0.46159926657980227 \n",
      "\t86 \t\t 0.4607345148737868 \n",
      "\t87 \t\t 0.4598794959789934 \n",
      "\t88 \t\t 0.4590339791333142 \n",
      "\t89 \t\t 0.4581977414683481 \n",
      "\t90 \t\t 0.45737056767455375 \n",
      "\t91 \t\t 0.4565522496827505 \n",
      "\t92 \t\t 0.4557425863610968 \n",
      "\t93 \t\t 0.4549413832265846 \n",
      "\t94 \t\t 0.45414845217039923 \n",
      "\t95 \t\t 0.453363611196244 \n",
      "\t96 \t\t 0.452586684171103 \n",
      "\t97 \t\t 0.451817500587591 \n",
      "\t98 \t\t 0.45105589533743606 \n",
      "\t99 \t\t 0.45030170849543766 \n",
      "\t100 \t\t 0.44955478511333835 \n"
     ]
    }
   ],
   "source": [
    "rec.fit(iters=100, latent_features=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### investigate the user-Item matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get some Infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nb of users: 4258\n",
      "Nb of items: 437\n",
      "The user_id with the highest avg rating given: 1102\n",
      "The article_id with the highest avg rating received: 50\n",
      "The article name with the highest avg rating received: Graph-based machine learning\n",
      "Shape of the U matrix: (4258, 5)\n",
      "Shape of the V(transpose) matrix: (5, 437)\n"
     ]
    }
   ],
   "source": [
    "def info():\n",
    "    user_item = user_item_df\n",
    "    nb_user = rec.n_users\n",
    "    nb_items = rec.n_items\n",
    "    item_name = rec.item_name_colname\n",
    "    item_id = rec.item_id_colname\n",
    "    u_mat = rec.user_mat\n",
    "    i_mat = rec.item_mat\n",
    "    user_high_rate = list(dict(user_item.mean(axis=1).sort_values(ascending=False).head(1)).keys())[0]\n",
    "    movie_id_high_rate = list(dict(user_item.mean(axis=0).sort_values(ascending=False).head(1)).keys())[0]\n",
    "    movie_name_high_rate = tuple(df_content[df_content[item_id] == movie_id_high_rate][item_name])[0]\n",
    "    \n",
    "\n",
    "    print(f\"Nb of users: {nb_user}\")\n",
    "    print(f\"Nb of items: {nb_items}\")\n",
    "    print(f\"The user_id with the highest avg rating given: {user_high_rate}\")\n",
    "    print(f\"The article_id with the highest avg rating received: {movie_id_high_rate}\")\n",
    "    print(f\"The article name with the highest avg rating received: {movie_name_high_rate}\")\n",
    "    print(f\"Shape of the U matrix: {u_mat.shape}\")\n",
    "    print(f\"Shape of the V(transpose) matrix: {i_mat.shape}\")\n",
    "\n",
    "info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "##### To make recommendation: given a user id we want to find similar users to this user in order to recommend items. Similarity are found by computing the dot product of characteristics of the user with its transpose, the more the result of an user-user pair is high, the more they have in common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_similarity = user_item_df.reset_index().replace(np.nan, 0)\n",
    "def prep_get_similar_user():\n",
    "    user_content = np.array(df_user_similarity.iloc[:,1:])\n",
    "    user_content_transpose = np.transpose(user_content)\n",
    "    dot_prod = user_content.dot(user_content_transpose)\n",
    "    return dot_prod\n",
    "\n",
    "dot_product_matrix_user = prep_get_similar_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare a function to display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_recommendations(rec_ids, rec_names, message, rec_ids_users, rec_user_articles):\n",
    "    \n",
    "    if type(rec_ids) == type(None):\n",
    "        print(f\"{message}\")\n",
    "    \n",
    "    else:\n",
    "        dict_id_name = dict(zip(rec_ids, rec_names))\n",
    "        \n",
    "        if type(rec_ids_users) != type(None):\n",
    "            print('Matrix Factorisation SVD:')\n",
    "            print(f\"\\t{message}\")\n",
    "            \n",
    "            for key, val  in dict_id_name.items():\n",
    "                print(f\"\\t- ID items: {key}\")\n",
    "                print(f\"\\tName: {val}\\n\")\n",
    "\n",
    "            print('CF User Based:')\n",
    "            print('\\tUser that are similar to you also seen:\\n')\n",
    "            for i in rec_user_articles[:5]:\n",
    "                print(f\"\\t- {i}\")\n",
    "        else:\n",
    "            print(f\"\\t{message}\")\n",
    "            dict_id_name = dict(zip(rec_ids, rec_names))\n",
    "            for key, val  in dict_id_name.items():\n",
    "                print(f\"\\t- ID items: {key}\")\n",
    "                print(f\"\\tName: {val}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### Existing user\n",
    "\n",
    "- Because it is an existing user, recommendations are made using FunkSVD (matrix factorisation), it will predict the rating it will give to all items and get back the items associate with the the top predicted rate. The dot product matrix is used here in order to find what other users similar to this user have seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Factorisation SVD:\n",
      "\tGlad to see you again! recommended for you:\n",
      "\n",
      "\t- ID items: 50\n",
      "\tName: Graph-based machine learning\n",
      "\n",
      "\t- ID items: 437\n",
      "\tName: IBM Watson Machine Learning: Get Started\n",
      "\n",
      "\t- ID items: 232\n",
      "\tName: Self-service data preparation with IBM Data Refinery\n",
      "\n",
      "\t- ID items: 534\n",
      "\tName: dplyr 0.5.0\n",
      "\n",
      "\t- ID items: 221\n",
      "\tName: How smart catalogs can turn the big data flood into an ocean of opportunity\n",
      "\n",
      "CF User Based:\n",
      "\tUser that are similar to you also seen:\n",
      "\n",
      "\t- upload files to ibm data science experience using the command line\n"
     ]
    }
   ],
   "source": [
    "rec_ids, rec_names, message, rec_ids_users, rec_user_articles = rec.make_recommendations(_id=3,\n",
    "                                                                                         dot_prod_user= dot_product_matrix_user,\n",
    "                                                                                         tfidf_matrix=tfidf_matrix,\n",
    "                                                                                         _id_type='user',\n",
    "                                                                                         rec_num=5)\n",
    "display_recommendations(rec_ids, rec_names, message, rec_ids_users, rec_user_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### New User\n",
    "\n",
    "- Because it is new user, recommendations are given using ranked based method, which simply return back the most popular items according to the ratings given by users, the number of ratings, the recency of the ratings. The dot_product_matrix will not be used but is requiered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked Based:\n",
      "\tHey, you are new here, this is for you:\n",
      "\n",
      "\t- ID items: 221\n",
      "\tName: How smart catalogs can turn the big data flood into an ocean of opportunity\n",
      "\n",
      "\t- ID items: 50\n",
      "\tName: Graph-based machine learning\n",
      "\n",
      "\t- ID items: 232\n",
      "\tName: Self-service data preparation with IBM Data Refinery\n",
      "\n",
      "\t- ID items: 43\n",
      "\tName: Deep Learning With Tensorflow Course by Big Data University\n",
      "\n",
      "\t- ID items: 732\n",
      "\tName: Rapidly build Machine Learning flows with DSX\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rec_ids, rec_names, message, rec_ids_users, rec_user_articles = rec.make_recommendations(_id=8000,\n",
    "                                                                                         dot_prod_user= dot_product_matrix_user,\n",
    "                                                                                         tfidf_matrix=tfidf_matrix,\n",
    "                                                                                         _id_type='user',\n",
    "                                                                                         rec_num=5)\n",
    "\n",
    "print('Ranked Based:')\n",
    "display_recommendations(rec_ids, rec_names, message, rec_ids_users, rec_user_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "#### Existing items\n",
    "\n",
    "- Here we enter an item id and would like to find similar items using the tfidf matrice you've computed before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content Based:\n",
      "\tSimilar items for id:593, corresponding to Upload Files to IBM Data Science Experience Using the Command Line:\n",
      "\n",
      "\t- ID items: 593\n",
      "\tName: Upload Files to IBM Data Science Experience Using the Command Line\n",
      "\n",
      "\t- ID items: 600\n",
      "\tName: Access IBM Analytics for Apache Spark from RStudio\n",
      "\n",
      "\t- ID items: 809\n",
      "\tName: Use the Machine Learning Library\n",
      "\n",
      "\t- ID items: 161\n",
      "\tName: Use the Machine Learning Library in Spark\n",
      "\n",
      "\t- ID items: 893\n",
      "\tName: Use the Machine Learning Library in IBM Analytics for Apache Spark\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rec_ids, rec_names, message, rec_ids_users, rec_user_articles = rec.make_recommendations(_id=593,\n",
    "                                                                                         dot_prod_user= dot_product_matrix_user,\n",
    "                                                                                         tfidf_matrix=tfidf_matrix,\n",
    "                                                                                         _id_type='item',\n",
    "                                                                                         rec_num=5)\n",
    "print('Content Based:')\n",
    "display_recommendations(rec_ids, rec_names, message, rec_ids_users, rec_user_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulate an error by passing a non existing item id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We can't make recommendation for this item, please makesure the data was updated with this item.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rec_ids, rec_names, message, rec_ids_users, rec_user_articles = rec.make_recommendations(_id=1087630,\n",
    "                                                                                         dot_prod_user= dot_product_matrix_user,\n",
    "                                                                                         tfidf_matrix=tfidf_matrix,\n",
    "                                                                                         _id_type='item',\n",
    "                                                                                         rec_num=5)\n",
    "\n",
    "\n",
    "display_recommendations(rec_ids, rec_names, message, rec_ids_users, rec_user_articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imagine that we have an article that we want to promote, but we want the top 10 users who may interested by this offer, the item id is 984."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These users may interested by this item:\n",
      "- 4165\n",
      "- 4184\n",
      "- 674\n",
      "- 2820\n",
      "- 3652\n",
      "- 1186\n",
      "- 4255\n",
      "- 3061\n",
      "- 3331\n",
      "- 182\n"
     ]
    }
   ],
   "source": [
    "user_item = user_item_df\n",
    "def may_interested_by(item_id, top_n=10):\n",
    "    pred = {}\n",
    "    # iterate over each users and predict the rate it will give to this movie\n",
    "    for user in user_item.index:\n",
    "        pred[user] = rec.predict_rating(user_id=user, item_id=item_id)\n",
    "\n",
    "    top_10_pairs = sorted(pred.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
    "    top_10_user_ids = []\n",
    "\n",
    "    for i in top_10_pairs:\n",
    "        top_10_user_ids.append(i[0])\n",
    "\n",
    "    return top_10_user_ids\n",
    "\n",
    "print('These users may interested by this item:')\n",
    "for i in may_interested_by(984, 10):\n",
    "    print(f\"- {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
